---
phase: 02-indexing-pipeline
plan: 02
type: execute
wave: 2
depends_on: ["02-01"]
files_modified:
  - src/cocosearch/indexer/embedder.py
  - src/cocosearch/indexer/flow.py
  - src/cocosearch/indexer/__init__.py
autonomous: true

must_haves:
  truths:
    - "CocoIndex flow processes files from a directory"
    - "Tree-sitter chunks code at semantic boundaries (functions, classes)"
    - "Ollama generates 768-dim embeddings for each chunk"
    - "Chunks are stored in PostgreSQL with vector index"
    - "Flow name includes index name to prevent collisions"
  artifacts:
    - path: "src/cocosearch/indexer/embedder.py"
      provides: "Shared embedding transform for index and query consistency"
      exports: ["code_to_embedding"]
    - path: "src/cocosearch/indexer/flow.py"
      provides: "CocoIndex flow definition for code indexing"
      exports: ["create_code_index_flow", "run_index"]
  key_links:
    - from: "src/cocosearch/indexer/flow.py"
      to: "cocoindex.sources.LocalFile"
      via: "File source with patterns"
      pattern: "LocalFile"
    - from: "src/cocosearch/indexer/flow.py"
      to: "cocoindex.functions.SplitRecursively"
      via: "Tree-sitter chunking"
      pattern: "SplitRecursively"
    - from: "src/cocosearch/indexer/embedder.py"
      to: "cocoindex.functions.EmbedText"
      via: "Ollama embedding generation"
      pattern: "EmbedText.*OLLAMA"
    - from: "src/cocosearch/indexer/flow.py"
      to: "cocoindex.storages.Postgres"
      via: "Vector storage export"
      pattern: "storages\\.Postgres"
---

<objective>
Implement the core CocoIndex flow for code indexing with Tree-sitter chunking and Ollama embeddings.

Purpose: This is the heart of the indexing pipeline - the CocoIndex flow that processes files, chunks them semantically using Tree-sitter, generates embeddings via Ollama, and stores results in PostgreSQL with vector indexes. Covers INDEX-01, INDEX-02, and INDEX-05 (incremental via CocoIndex's built-in state tracking).

Output: Working indexing flow that can process a codebase directory and store embeddings in PostgreSQL.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-indexing-pipeline/02-RESEARCH.md
@.planning/phases/02-indexing-pipeline/02-CONTEXT.md
@.planning/phases/02-indexing-pipeline/02-01-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create shared embedding transform</name>
  <files>src/cocosearch/indexer/embedder.py</files>
  <action>
Create `embedder.py` with a shared embedding function that will be used by both indexing and search (Phase 3) to ensure consistent embeddings.

Implement:

1. `@cocoindex.transform_flow()` decorated function `code_to_embedding`:
   - Takes `text: cocoindex.DataSlice[str]`
   - Returns `cocoindex.DataSlice[list[float]]`
   - Uses `cocoindex.functions.EmbedText` with:
     - api_type=cocoindex.LlmApiType.OLLAMA
     - model="nomic-embed-text"

2. Helper function `extract_extension(filename: str) -> str`:
   - Decorated with `@cocoindex.op.function()`
   - Returns file extension (e.g., ".py" -> "py", handles no extension)
   - Used for language detection in Tree-sitter

Export both from the module.

Reference the research document Pattern 3 for the exact implementation.
  </action>
  <verify>
Run:
```
uv run python -c "
from cocosearch.indexer.embedder import code_to_embedding, extract_extension
print(f'code_to_embedding: {code_to_embedding}')
print(f'extract_extension(\"test.py\"): {extract_extension(\"test.py\")}')
"
```
Should print the function references and "py" for the extension.
  </verify>
  <done>Shared embedding transform created using @cocoindex.transform_flow decorator with Ollama/nomic-embed-text configuration.</done>
</task>

<task type="auto">
  <name>Task 2: Create CocoIndex flow definition</name>
  <files>src/cocosearch/indexer/flow.py, src/cocosearch/indexer/__init__.py</files>
  <action>
Create `flow.py` with the main indexing flow.

Implement:

1. `create_code_index_flow` function that returns a flow definition:
   - Takes parameters: index_name, codebase_path, include_patterns, exclude_patterns
   - Uses `@cocoindex.flow_def(name=f"CodeIndex_{index_name}")` decorator pattern
   - IMPORTANT: Flow name MUST include index_name to prevent collisions between indexes

2. Inside the flow:
   a. Add LocalFile source:
      - path=codebase_path
      - included_patterns from parameter
      - excluded_patterns from parameter
      - binary_as_text=False (skip binary files)

   b. Create collector for embeddings

   c. For each file:
      - Extract extension using extract_extension helper
      - Chunk using SplitRecursively():
        - language=file extension
        - chunk_size=1000 (from config, passed in)
        - chunk_overlap=300 (from config, passed in)

   d. For each chunk:
      - Generate embedding using code_to_embedding transform
      - Collect with metadata:
        - filename (relative path)
        - location (contains line/column info from SplitRecursively)
        - embedding (vector)

   e. Export to Postgres:
      - Table name: f"{index_name}_chunks"
      - primary_key_fields=["filename", "location"]
      - vector_indexes with COSINE_SIMILARITY on "embedding"

3. `run_index` function:
   - Takes: index_name, codebase_path, config (IndexingConfig)
   - Initializes cocoindex if not already (cocoindex.init())
   - Builds exclude patterns using build_exclude_patterns
   - Creates the flow
   - Runs flow.setup() then flow.update()
   - Returns update statistics

Update __init__.py to export: create_code_index_flow, run_index, code_to_embedding

IMPORTANT per CONTEXT.md: Store only references (filename + location), not full chunk text. The location from SplitRecursively contains line/column info.
  </action>
  <verify>
Run (requires PostgreSQL and Ollama running):
```
uv run python -c "
from cocosearch.indexer import run_index, IndexingConfig
# Just verify imports work - actual run needs infra
print('Imports OK')
"
```

Full integration test (if infra running):
```
# Create test directory
mkdir -p /tmp/test_codebase
echo 'def hello(): return \"world\"' > /tmp/test_codebase/test.py

# Run index
uv run python -c "
from cocosearch.indexer import run_index, IndexingConfig
import cocoindex
cocoindex.init()
stats = run_index('test_index', '/tmp/test_codebase', IndexingConfig())
print(f'Stats: {stats}')
"
```
  </verify>
  <done>CocoIndex flow definition created with Tree-sitter chunking, Ollama embeddings, and PostgreSQL vector storage. Flow name includes index_name for isolation.</done>
</task>

</tasks>

<verification>
After all tasks complete:

1. Embedder module exists and exports:
   ```bash
   uv run python -c "from cocosearch.indexer import code_to_embedding; print('embedder OK')"
   ```

2. Flow module exists and exports:
   ```bash
   uv run python -c "from cocosearch.indexer import run_index, create_code_index_flow; print('flow OK')"
   ```

3. Full integration test (requires running infrastructure):
   ```bash
   # Ensure PostgreSQL and Ollama are running
   docker compose up -d

   # Create test codebase
   mkdir -p /tmp/cocosearch_test
   cat > /tmp/cocosearch_test/sample.py << 'EOF'
   def greet(name: str) -> str:
       """Return a greeting message."""
       return f"Hello, {name}!"

   class Calculator:
       def add(self, a: int, b: int) -> int:
           return a + b
   EOF

   # Run indexing
   uv run python -c "
   from cocosearch.indexer import run_index, IndexingConfig
   import cocoindex
   cocoindex.init()
   stats = run_index('integration_test', '/tmp/cocosearch_test', IndexingConfig())
   print(f'Indexing complete: {stats}')
   "
   ```

4. Verify data in PostgreSQL:
   ```bash
   docker exec -it coco-s-db-1 psql -U cocoindex -d cocoindex -c "SELECT COUNT(*) FROM integration_test_chunks;"
   ```
</verification>

<success_criteria>
- code_to_embedding transform created with @cocoindex.transform_flow decorator
- extract_extension helper correctly extracts file extensions
- CocoIndex flow processes files with Tree-sitter chunking (SplitRecursively)
- Embeddings generated via Ollama nomic-embed-text
- Chunks stored in PostgreSQL with vector index on embedding column
- Flow name includes index_name to prevent collisions
- run_index() orchestrates the full indexing process
</success_criteria>

<output>
After completion, create `.planning/phases/02-indexing-pipeline/02-02-SUMMARY.md`
</output>
