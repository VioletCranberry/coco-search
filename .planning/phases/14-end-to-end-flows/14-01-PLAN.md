---
phase: 14-end-to-end-flows
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - tests/fixtures/e2e_fixtures/auth.py
  - tests/fixtures/e2e_fixtures/main.tf
  - tests/fixtures/e2e_fixtures/Dockerfile
  - tests/fixtures/e2e_fixtures/deploy.sh
  - tests/fixtures/e2e_fixtures/utils.js
  - tests/integration/test_e2e_indexing.py
autonomous: true

must_haves:
  truths:
    - "CLI index command indexes files into PostgreSQL with real embeddings"
    - "Indexed files can be retrieved from database with correct metadata"
    - "Incremental indexing only processes changed files"
  artifacts:
    - path: "tests/fixtures/e2e_fixtures/"
      provides: "Minimal synthetic test codebase"
      min_lines: 50
    - path: "tests/integration/test_e2e_indexing.py"
      provides: "E2E indexing flow tests"
      exports: ["test_full_indexing_flow", "test_incremental_indexing"]
  key_links:
    - from: "tests/integration/test_e2e_indexing.py"
      to: "cocosearch CLI"
      via: "subprocess.run() with environment propagation"
      pattern: "subprocess\\.run.*cocosearch.*index"
---

<objective>
Create test fixtures and E2E indexing flow tests that validate the complete pipeline from file discovery through embedding generation to vector storage.

Purpose: Establish the foundation for E2E testing with minimal synthetic fixtures and prove the indexing flow works end-to-end with real PostgreSQL and Ollama.

Output: Test codebase fixture directory and indexing integration tests covering requirements E2E-01 and E2E-03.
</objective>

<execution_context>
@/Users/fedorzhdanov/.claude/get-shit-done/workflows/execute-plan.md
@/Users/fedorzhdanov/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/14-end-to-end-flows/14-CONTEXT.md
@.planning/phases/14-end-to-end-flows/14-RESEARCH.md

# Existing container fixtures
@tests/fixtures/containers.py
@tests/fixtures/ollama_integration.py

# Integration test patterns
@tests/integration/conftest.py
@tests/integration/test_postgresql.py

# CLI implementation
@src/cocosearch/cli.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create minimal synthetic test codebase fixture</name>
  <files>
    tests/fixtures/e2e_fixtures/__init__.py
    tests/fixtures/e2e_fixtures/auth.py
    tests/fixtures/e2e_fixtures/main.tf
    tests/fixtures/e2e_fixtures/Dockerfile
    tests/fixtures/e2e_fixtures/deploy.sh
    tests/fixtures/e2e_fixtures/utils.js
  </files>
  <action>
Create a minimal synthetic test codebase with 5-6 files covering Python, JavaScript, Terraform, Dockerfile, and Bash. Each file should have:

1. **Predictable search terms** - Include specific keywords for reliable assertions (e.g., "authenticate_user", "aws_instance", "docker build")
2. **Realistic structure** - Not toy examples but realistic snippets (10-30 lines each)
3. **Language-specific patterns** - Use idiomatic code for each language

File contents (use exactly these for predictable testing):

**auth.py** (~20 lines):
```python
"""User authentication module for the application."""

def authenticate_user(username: str, password: str) -> bool:
    """Authenticate a user with credentials.

    Validates username and password against the user database.
    Returns True if authentication succeeds.
    """
    if not username or not password:
        return False

    # Verify against database
    user = get_user_by_username(username)
    if user is None:
        return False

    return verify_password(password, user.password_hash)
```

**main.tf** (~15 lines):
```hcl
resource "aws_instance" "web_server" {
  ami           = "ami-0c55b159cbfafe1f0"
  instance_type = "t2.micro"

  tags = {
    Name        = "WebServer"
    Environment = "production"
  }
}

output "instance_ip" {
  value = aws_instance.web_server.public_ip
}
```

**Dockerfile** (~15 lines):
```dockerfile
FROM python:3.11-slim

WORKDIR /app

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY . .

EXPOSE 8000
CMD ["python", "-m", "uvicorn", "app:main", "--host", "0.0.0.0"]
```

**deploy.sh** (~15 lines):
```bash
#!/bin/bash
set -euo pipefail

echo "Deploying application to production..."

docker build -t myapp:latest .
docker push registry.example.com/myapp:latest

kubectl apply -f k8s/deployment.yaml
kubectl rollout status deployment/myapp
```

**utils.js** (~20 lines):
```javascript
/**
 * Utility functions for data processing.
 */

function formatCurrency(amount, currency = 'USD') {
    return new Intl.NumberFormat('en-US', {
        style: 'currency',
        currency: currency
    }).format(amount);
}

function debounce(func, wait) {
    let timeout;
    return function executedFunction(...args) {
        clearTimeout(timeout);
        timeout = setTimeout(() => func.apply(this, args), wait);
    };
}

module.exports = { formatCurrency, debounce };
```

Also create an empty `__init__.py` to make it a proper Python package.
  </action>
  <verify>
Verify directory structure exists:
```bash
ls -la tests/fixtures/e2e_fixtures/
```
Expected: 6 files (auth.py, main.tf, Dockerfile, deploy.sh, utils.js, __init__.py)
  </verify>
  <done>Minimal synthetic test codebase exists with 5-6 files covering Python, JavaScript, Terraform, Dockerfile, and Bash with predictable search terms</done>
</task>

<task type="auto">
  <name>Task 2: Create E2E indexing flow tests</name>
  <files>tests/integration/test_e2e_indexing.py</files>
  <action>
Create integration tests for the full indexing flow using subprocess to invoke the actual CLI.

Key patterns (from 14-RESEARCH.md):
1. **Environment propagation** - Copy os.environ and add COCOINDEX_DATABASE_URL and OLLAMA_HOST
2. **subprocess.run()** - Use sys.executable to invoke cocosearch module
3. **Exit code validation** - Assert returncode == 0 before any output parsing
4. **Unique index names** - Use test function name for isolation

Tests to implement:

**test_full_indexing_flow()** (E2E-01, E2E-03):
- Index the e2e_fixtures codebase via CLI
- Verify exit code is 0
- Query database to verify files were indexed
- Check chunk count matches expected (should be >= 5, one per file minimum)

**test_incremental_indexing()** (validates incremental behavior):
- Create temp codebase with 2 files
- Index once, record state
- Modify one file
- Re-index
- Verify only modified file was re-processed (via CLI output or stats)

**test_index_nonexistent_path()** (error handling):
- Attempt to index a path that doesn't exist
- Verify exit code is non-zero
- Verify error message is helpful

Use pytest fixtures:
- `initialized_db` - PostgreSQL with pgvector extension
- `warmed_ollama` - Pre-warmed Ollama service
- `tmp_path` - For temporary codebase creation

Helper function to run CLI:
```python
def run_cocosearch(args: list[str], env: dict) -> subprocess.CompletedProcess:
    """Run cocosearch CLI with given arguments."""
    return subprocess.run(
        [sys.executable, "-m", "cocosearch"] + args,
        capture_output=True,
        text=True,
        env=env,
    )
```

Fixture to get e2e_fixtures path:
```python
@pytest.fixture
def e2e_fixtures_path():
    """Return path to e2e test fixtures."""
    return Path(__file__).parent.parent / "fixtures" / "e2e_fixtures"
```
  </action>
  <verify>
Run the integration tests:
```bash
cd /Users/fedorzhdanov/GIT/personal/coco-s && uv run pytest tests/integration/test_e2e_indexing.py -v -m integration 2>&1 | head -50
```
Expected: All 3 tests pass (test_full_indexing_flow, test_incremental_indexing, test_index_nonexistent_path)
  </verify>
  <done>E2E indexing tests validate CLI index command with real PostgreSQL and Ollama, including full flow, incremental indexing, and error handling</done>
</task>

</tasks>

<verification>
1. Test fixtures directory exists with 5-6 files
2. All E2E indexing tests pass with real containers
3. Index command creates data in PostgreSQL
4. Incremental indexing correctly detects changes
5. Error handling provides helpful messages
</verification>

<success_criteria>
- Minimal synthetic test codebase exists at tests/fixtures/e2e_fixtures/
- test_e2e_indexing.py contains at least 3 tests
- `pytest tests/integration/test_e2e_indexing.py -v -m integration` shows all tests passing
- Requirements E2E-01 and E2E-03 are satisfied
</success_criteria>

<output>
After completion, create `.planning/phases/14-end-to-end-flows/14-01-SUMMARY.md`
</output>
