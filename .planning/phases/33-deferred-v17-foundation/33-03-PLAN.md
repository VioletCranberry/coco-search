---
phase: 33-deferred-v17-foundation
plan: 03
type: execute
wave: 2
depends_on: ["33-01"]
files_modified:
  - src/cocosearch/search/cache.py
  - src/cocosearch/search/query.py
  - src/cocosearch/indexer/flow.py
  - src/cocosearch/cli.py
  - tests/test_cache.py
autonomous: true

must_haves:
  truths:
    - "Repeated identical queries return cached results (sub-10ms after first call)"
    - "Semantic cache hits similar queries (cosine >0.95 reuses embeddings)"
    - "Cache invalidates automatically when reindex runs"
    - "--no-cache flag bypasses cache entirely"
  artifacts:
    - path: "src/cocosearch/search/cache.py"
      provides: "Query cache module with exact and semantic caching"
      exports: ["QueryCache", "get_query_cache", "invalidate_index_cache"]
    - path: "src/cocosearch/cli.py"
      provides: "CLI with --no-cache flag"
      contains: "--no-cache"
    - path: "tests/test_cache.py"
      provides: "Unit tests for cache module"
      exports: ["test_exact_cache_hit", "test_semantic_cache_hit"]
  key_links:
    - from: "src/cocosearch/search/query.py"
      to: "src/cocosearch/search/cache.py"
      via: "cache lookup before embedding/search"
      pattern: "cache\\.get"
    - from: "src/cocosearch/indexer/flow.py"
      to: "src/cocosearch/search/cache.py"
      via: "invalidate_index_cache call"
      pattern: "invalidate_index_cache"
---

<objective>
Implement two-level query caching (exact match + semantic similarity) to improve response times for repeated and similar queries.

Purpose: Embedding generation is the main latency source (~300-500ms per query via Ollama). Caching exact query matches and semantically similar queries can reduce response time to sub-10ms for cache hits, significantly improving interactive search experience.

Output: New cache.py module, updated query.py with cache integration, cache invalidation in indexing flow, --no-cache CLI flag, and comprehensive unit tests.
</objective>

<execution_context>
@/Users/fedorzhdanov/.claude/get-shit-done/workflows/execute-plan.md
@/Users/fedorzhdanov/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/33-deferred-v17-foundation/33-RESEARCH.md

Reference:
@src/cocosearch/search/query.py
@src/cocosearch/indexer/flow.py
@src/cocosearch/cli.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create cache.py module with two-level caching</name>
  <files>src/cocosearch/search/cache.py</files>
  <action>
Create new cache module implementing hash-based exact match caching and embedding-based semantic similarity caching:

```python
"""Query cache module for cocosearch.

Implements two-level caching for search queries:
1. Exact match: Hash-based lookup for identical queries
2. Semantic: Embedding similarity for paraphrased queries (cosine > 0.95)

Cache is persistent (survives restarts) and invalidates on reindex.
"""

import hashlib
import logging
import os
import time
from dataclasses import dataclass
from functools import lru_cache
from typing import Any

import numpy as np

logger = logging.getLogger(__name__)

# Default cache directory (under user home)
DEFAULT_CACHE_DIR = os.path.expanduser("~/.cache/cocosearch/queries")

# Cache settings
CACHE_SIZE_LIMIT = 1_000_000_000  # 1GB
DEFAULT_TTL = 86400  # 24 hours
SEMANTIC_THRESHOLD = 0.95  # Cosine similarity threshold for semantic cache hits


@dataclass
class CacheEntry:
    """A cached query result with metadata."""
    results: list[Any]  # SearchResult objects
    embedding: list[float] | None  # Query embedding for semantic matching
    timestamp: float
    index_name: str


def _compute_cache_key(
    query: str,
    index_name: str,
    limit: int,
    min_score: float,
    language_filter: str | None,
    use_hybrid: bool | None,
    symbol_type: str | list[str] | None,
    symbol_name: str | None,
) -> str:
    """Compute SHA256 hash key from query parameters.

    Uses all search parameters to ensure cache hits only for identical searches.

    Args:
        query: Search query text.
        index_name: Index being searched.
        limit: Max results.
        min_score: Minimum score threshold.
        language_filter: Language filter if any.
        use_hybrid: Hybrid search flag.
        symbol_type: Symbol type filter.
        symbol_name: Symbol name filter.

    Returns:
        SHA256 hex digest as cache key.
    """
    # Normalize symbol_type to sorted tuple for consistent hashing
    if isinstance(symbol_type, list):
        symbol_type_str = ",".join(sorted(symbol_type))
    elif symbol_type:
        symbol_type_str = symbol_type
    else:
        symbol_type_str = ""

    # Build deterministic key string
    key_parts = [
        f"query={query}",
        f"index={index_name}",
        f"limit={limit}",
        f"min_score={min_score}",
        f"language={language_filter or ''}",
        f"hybrid={use_hybrid}",
        f"symbol_type={symbol_type_str}",
        f"symbol_name={symbol_name or ''}",
    ]
    key_str = "|".join(key_parts)

    return hashlib.sha256(key_str.encode()).hexdigest()


def cosine_similarity(a: list[float], b: list[float]) -> float:
    """Compute cosine similarity between two embeddings.

    Args:
        a: First embedding vector.
        b: Second embedding vector.

    Returns:
        Cosine similarity in range [-1, 1], typically [0, 1] for embeddings.
    """
    a_np = np.array(a)
    b_np = np.array(b)

    dot = np.dot(a_np, b_np)
    norm_a = np.linalg.norm(a_np)
    norm_b = np.linalg.norm(b_np)

    if norm_a == 0 or norm_b == 0:
        return 0.0

    return float(dot / (norm_a * norm_b))


class QueryCache:
    """Two-level query cache with exact and semantic matching.

    Level 1 (Exact): Hash-based lookup for identical queries
    Level 2 (Semantic): Embedding similarity for paraphrased queries

    Cache entries expire after TTL and are invalidated on reindex.
    """

    def __init__(
        self,
        cache_dir: str = DEFAULT_CACHE_DIR,
        ttl: int = DEFAULT_TTL,
        semantic_threshold: float = SEMANTIC_THRESHOLD,
    ):
        """Initialize the query cache.

        Args:
            cache_dir: Directory for persistent cache storage.
            ttl: Time-to-live in seconds (default 24 hours).
            semantic_threshold: Cosine similarity threshold for semantic hits.
        """
        self.cache_dir = cache_dir
        self.ttl = ttl
        self.semantic_threshold = semantic_threshold

        # In-memory cache for fast access (session-scoped)
        # Key: cache_key, Value: CacheEntry
        self._cache: dict[str, CacheEntry] = {}

        # Embedding index for semantic search (index_name -> list of (key, embedding))
        self._embedding_index: dict[str, list[tuple[str, list[float]]]] = {}

        # Ensure cache directory exists
        os.makedirs(cache_dir, exist_ok=True)

        logger.debug(f"Query cache initialized at {cache_dir}")

    def get(
        self,
        query: str,
        index_name: str,
        limit: int,
        min_score: float,
        language_filter: str | None,
        use_hybrid: bool | None,
        symbol_type: str | list[str] | None,
        symbol_name: str | None,
        query_embedding: list[float] | None = None,
    ) -> tuple[list[Any] | None, str]:
        """Look up query in cache (exact then semantic).

        Args:
            query: Search query text.
            index_name: Index being searched.
            limit: Max results.
            min_score: Minimum score threshold.
            language_filter: Language filter if any.
            use_hybrid: Hybrid search flag.
            symbol_type: Symbol type filter.
            symbol_name: Symbol name filter.
            query_embedding: Pre-computed embedding for semantic matching.

        Returns:
            Tuple of (results, hit_type) where:
            - results: Cached results or None if miss
            - hit_type: "exact", "semantic", or "miss"
        """
        cache_key = _compute_cache_key(
            query, index_name, limit, min_score,
            language_filter, use_hybrid, symbol_type, symbol_name,
        )

        # Level 1: Exact match
        if cache_key in self._cache:
            entry = self._cache[cache_key]
            # Check TTL
            if time.time() - entry.timestamp < self.ttl:
                logger.debug(f"Cache hit (exact): {cache_key[:16]}...")
                return entry.results, "exact"
            else:
                # Expired - remove from cache
                del self._cache[cache_key]
                self._remove_from_embedding_index(index_name, cache_key)

        # Level 2: Semantic match (only if we have embedding)
        if query_embedding and index_name in self._embedding_index:
            for key, cached_embedding in self._embedding_index[index_name]:
                if key in self._cache:
                    entry = self._cache[key]
                    # Check TTL
                    if time.time() - entry.timestamp >= self.ttl:
                        continue

                    sim = cosine_similarity(query_embedding, cached_embedding)
                    if sim >= self.semantic_threshold:
                        logger.debug(f"Cache hit (semantic, sim={sim:.3f}): {key[:16]}...")
                        return entry.results, "semantic"

        return None, "miss"

    def put(
        self,
        query: str,
        index_name: str,
        limit: int,
        min_score: float,
        language_filter: str | None,
        use_hybrid: bool | None,
        symbol_type: str | list[str] | None,
        symbol_name: str | None,
        results: list[Any],
        query_embedding: list[float] | None = None,
    ) -> None:
        """Store query results in cache.

        Args:
            query: Search query text.
            index_name: Index being searched.
            limit: Max results.
            min_score: Minimum score threshold.
            language_filter: Language filter if any.
            use_hybrid: Hybrid search flag.
            symbol_type: Symbol type filter.
            symbol_name: Symbol name filter.
            results: Search results to cache.
            query_embedding: Query embedding for semantic matching.
        """
        cache_key = _compute_cache_key(
            query, index_name, limit, min_score,
            language_filter, use_hybrid, symbol_type, symbol_name,
        )

        entry = CacheEntry(
            results=results,
            embedding=query_embedding,
            timestamp=time.time(),
            index_name=index_name,
        )

        self._cache[cache_key] = entry

        # Add to embedding index if we have embedding
        if query_embedding:
            if index_name not in self._embedding_index:
                self._embedding_index[index_name] = []
            self._embedding_index[index_name].append((cache_key, query_embedding))

        logger.debug(f"Cache put: {cache_key[:16]}...")

    def invalidate_index(self, index_name: str) -> int:
        """Remove all cached entries for an index.

        Called when reindexing to ensure fresh results.

        Args:
            index_name: Index to invalidate.

        Returns:
            Number of entries removed.
        """
        removed = 0

        # Remove from main cache
        keys_to_remove = [
            key for key, entry in self._cache.items()
            if entry.index_name == index_name
        ]
        for key in keys_to_remove:
            del self._cache[key]
            removed += 1

        # Remove from embedding index
        if index_name in self._embedding_index:
            del self._embedding_index[index_name]

        logger.info(f"Cache invalidated for index '{index_name}': {removed} entries removed")
        return removed

    def _remove_from_embedding_index(self, index_name: str, cache_key: str) -> None:
        """Remove a single entry from the embedding index."""
        if index_name in self._embedding_index:
            self._embedding_index[index_name] = [
                (k, e) for k, e in self._embedding_index[index_name]
                if k != cache_key
            ]

    def clear(self) -> None:
        """Clear all cached entries."""
        self._cache.clear()
        self._embedding_index.clear()
        logger.info("Cache cleared")


# Module-level singleton
_query_cache: QueryCache | None = None


def get_query_cache() -> QueryCache:
    """Get or create the global query cache singleton."""
    global _query_cache
    if _query_cache is None:
        _query_cache = QueryCache()
    return _query_cache


def invalidate_index_cache(index_name: str) -> int:
    """Invalidate cache for an index (convenience function).

    Args:
        index_name: Index to invalidate.

    Returns:
        Number of entries removed.
    """
    cache = get_query_cache()
    return cache.invalidate_index(index_name)
```

Note: This uses in-memory caching for the session. Persistent caching with diskcache can be added later if needed.
  </action>
  <verify>
Run: `python -c "from cocosearch.search.cache import get_query_cache; print('OK')"`
Module should import without errors.
  </verify>
  <done>
cache.py module exists with QueryCache class.
Implements exact hash-based caching and semantic similarity caching.
Exports get_query_cache() and invalidate_index_cache().
  </done>
</task>

<task type="auto">
  <name>Task 2: Integrate cache into search() function</name>
  <files>src/cocosearch/search/query.py</files>
  <action>
Update the search() function to check cache before executing search:

1. Add imports at top of file:
   ```python
   from cocosearch.search.cache import get_query_cache
   ```

2. Add `no_cache: bool = False` parameter to search() function signature.

3. At the start of search() (after validation, before embedding), add cache lookup:
   ```python
   # Check cache unless disabled
   if not no_cache:
       cache = get_query_cache()
       cached_results, hit_type = cache.get(
           query=query,
           index_name=index_name,
           limit=limit,
           min_score=min_score,
           language_filter=language_filter,
           use_hybrid=use_hybrid,
           symbol_type=symbol_type,
           symbol_name=symbol_name,
           query_embedding=None,  # Don't have embedding yet for semantic check
       )
       if cached_results is not None:
           logger.debug(f"Cache hit ({hit_type})")
           return cached_results
   ```

4. After getting results (before return), cache them:
   ```python
   # Cache results for future queries
   if not no_cache:
       cache = get_query_cache()
       cache.put(
           query=query,
           index_name=index_name,
           limit=limit,
           min_score=min_score,
           language_filter=language_filter,
           use_hybrid=use_hybrid,
           symbol_type=symbol_type,
           symbol_name=symbol_name,
           results=results,
           query_embedding=query_embedding if 'query_embedding' in dir() else None,
       )
   ```

5. For semantic cache hits (on subsequent queries), we need to check after embedding:
   - Move cache check for semantic to after embedding generation
   - Or: Do initial exact-only check, then after embedding do semantic check

Recommended approach: Two cache checks - exact before embedding, semantic after embedding (if exact missed).
  </action>
  <verify>
Run search twice with same query, check log for "Cache hit":
```bash
COCOSEARCH_LOG_LEVEL=DEBUG cocosearch search "process data" -i test
COCOSEARCH_LOG_LEVEL=DEBUG cocosearch search "process data" -i test
```
Second query should show cache hit in debug output.
  </verify>
  <done>
search() function has cache lookup before search execution.
search() function caches results after successful search.
Repeated identical queries return from cache.
no_cache parameter added to bypass caching.
  </done>
</task>

<task type="auto">
  <name>Task 3: Add cache invalidation to indexing flow</name>
  <files>src/cocosearch/indexer/flow.py, src/cocosearch/cli.py</files>
  <action>
1. In flow.py, add cache invalidation at the START of run_index():
   ```python
   from cocosearch.search.cache import invalidate_index_cache

   def run_index(...):
       # Invalidate query cache for this index before reindexing
       # This ensures stale results aren't served during/after reindex
       try:
           removed = invalidate_index_cache(index_name)
           if removed > 0:
               logger.info(f"Invalidated {removed} cached queries for index '{index_name}'")
       except Exception as e:
           logger.warning(f"Cache invalidation failed (non-fatal): {e}")

       # ... rest of existing code ...
   ```

2. In cli.py, add --no-cache flag to search command:
   - Find the search command definition
   - Add option: `@click.option("--no-cache", is_flag=True, help="Bypass query cache")`
   - Pass to search() function call

3. Update CLI search command to pass no_cache parameter:
   ```python
   results = search(
       query=query,
       index_name=index,
       limit=limit,
       min_score=min_score,
       language_filter=language,
       use_hybrid=hybrid,
       symbol_type=symbol_type,
       symbol_name=symbol_name,
       no_cache=no_cache,  # Add this
   )
   ```
  </action>
  <verify>
1. Run indexing and check logs for cache invalidation message:
   ```bash
   cocosearch index ./src test-index
   ```
2. Run search with --no-cache flag:
   ```bash
   cocosearch search --no-cache "process data" -i test-index
   ```
  </verify>
  <done>
run_index() invalidates cache for index before reindexing.
CLI has --no-cache flag that bypasses cache.
Cache invalidation logs when entries are removed.
  </done>
</task>

<task type="auto">
  <name>Task 4: Create unit tests for cache module</name>
  <files>tests/test_cache.py</files>
  <action>
Create comprehensive unit tests for the cache module:

```python
"""Unit tests for query cache module.

Tests exact match caching, semantic similarity caching,
cache invalidation, and TTL expiration.
"""

import time
import pytest
from unittest.mock import patch

from cocosearch.search.cache import (
    QueryCache,
    _compute_cache_key,
    cosine_similarity,
    get_query_cache,
    invalidate_index_cache,
)


class TestCacheKey:
    """Tests for cache key computation."""

    def test_same_params_same_key(self):
        """Identical parameters produce identical keys."""
        key1 = _compute_cache_key(
            "query", "index", 10, 0.0, None, None, None, None
        )
        key2 = _compute_cache_key(
            "query", "index", 10, 0.0, None, None, None, None
        )
        assert key1 == key2

    def test_different_query_different_key(self):
        """Different queries produce different keys."""
        key1 = _compute_cache_key(
            "query1", "index", 10, 0.0, None, None, None, None
        )
        key2 = _compute_cache_key(
            "query2", "index", 10, 0.0, None, None, None, None
        )
        assert key1 != key2

    def test_different_index_different_key(self):
        """Different indexes produce different keys."""
        key1 = _compute_cache_key(
            "query", "index1", 10, 0.0, None, None, None, None
        )
        key2 = _compute_cache_key(
            "query", "index2", 10, 0.0, None, None, None, None
        )
        assert key1 != key2

    def test_symbol_type_list_normalized(self):
        """Symbol type list is normalized for consistent hashing."""
        key1 = _compute_cache_key(
            "query", "index", 10, 0.0, None, None, ["function", "method"], None
        )
        key2 = _compute_cache_key(
            "query", "index", 10, 0.0, None, None, ["method", "function"], None
        )
        assert key1 == key2  # Order shouldn't matter


class TestCosineSimilarity:
    """Tests for cosine similarity computation."""

    def test_identical_vectors(self):
        """Identical vectors have similarity 1.0."""
        vec = [1.0, 0.0, 0.0]
        assert cosine_similarity(vec, vec) == pytest.approx(1.0)

    def test_orthogonal_vectors(self):
        """Orthogonal vectors have similarity 0.0."""
        vec1 = [1.0, 0.0, 0.0]
        vec2 = [0.0, 1.0, 0.0]
        assert cosine_similarity(vec1, vec2) == pytest.approx(0.0)

    def test_opposite_vectors(self):
        """Opposite vectors have similarity -1.0."""
        vec1 = [1.0, 0.0, 0.0]
        vec2 = [-1.0, 0.0, 0.0]
        assert cosine_similarity(vec1, vec2) == pytest.approx(-1.0)


class TestQueryCache:
    """Tests for QueryCache class."""

    @pytest.fixture
    def cache(self, tmp_path):
        """Create a cache instance with temp directory."""
        return QueryCache(cache_dir=str(tmp_path), ttl=3600)

    def test_exact_cache_hit(self, cache):
        """Exact match returns cached results."""
        results = [{"file": "test.py", "score": 0.9}]

        cache.put(
            query="test query",
            index_name="test-index",
            limit=10,
            min_score=0.0,
            language_filter=None,
            use_hybrid=None,
            symbol_type=None,
            symbol_name=None,
            results=results,
        )

        cached, hit_type = cache.get(
            query="test query",
            index_name="test-index",
            limit=10,
            min_score=0.0,
            language_filter=None,
            use_hybrid=None,
            symbol_type=None,
            symbol_name=None,
        )

        assert cached == results
        assert hit_type == "exact"

    def test_cache_miss(self, cache):
        """Missing query returns None."""
        cached, hit_type = cache.get(
            query="unknown query",
            index_name="test-index",
            limit=10,
            min_score=0.0,
            language_filter=None,
            use_hybrid=None,
            symbol_type=None,
            symbol_name=None,
        )

        assert cached is None
        assert hit_type == "miss"

    def test_semantic_cache_hit(self, cache):
        """Similar embedding returns cached results."""
        results = [{"file": "test.py", "score": 0.9}]
        embedding = [1.0, 0.0, 0.0]

        # Store with embedding
        cache.put(
            query="original query",
            index_name="test-index",
            limit=10,
            min_score=0.0,
            language_filter=None,
            use_hybrid=None,
            symbol_type=None,
            symbol_name=None,
            results=results,
            query_embedding=embedding,
        )

        # Query with very similar embedding (sim > 0.95)
        similar_embedding = [0.99, 0.01, 0.0]

        cached, hit_type = cache.get(
            query="different query",
            index_name="test-index",
            limit=10,
            min_score=0.0,
            language_filter=None,
            use_hybrid=None,
            symbol_type=None,
            symbol_name=None,
            query_embedding=similar_embedding,
        )

        assert cached == results
        assert hit_type == "semantic"

    def test_semantic_cache_miss_below_threshold(self, cache):
        """Dissimilar embedding returns miss."""
        results = [{"file": "test.py", "score": 0.9}]
        embedding = [1.0, 0.0, 0.0]

        cache.put(
            query="original query",
            index_name="test-index",
            limit=10,
            min_score=0.0,
            language_filter=None,
            use_hybrid=None,
            symbol_type=None,
            symbol_name=None,
            results=results,
            query_embedding=embedding,
        )

        # Very different embedding (sim < 0.95)
        different_embedding = [0.0, 1.0, 0.0]

        cached, hit_type = cache.get(
            query="different query",
            index_name="test-index",
            limit=10,
            min_score=0.0,
            language_filter=None,
            use_hybrid=None,
            symbol_type=None,
            symbol_name=None,
            query_embedding=different_embedding,
        )

        assert cached is None
        assert hit_type == "miss"

    def test_invalidate_index(self, cache):
        """Invalidation removes all entries for index."""
        results = [{"file": "test.py"}]

        cache.put(
            query="query1",
            index_name="test-index",
            limit=10,
            min_score=0.0,
            language_filter=None,
            use_hybrid=None,
            symbol_type=None,
            symbol_name=None,
            results=results,
        )

        removed = cache.invalidate_index("test-index")
        assert removed == 1

        cached, _ = cache.get(
            query="query1",
            index_name="test-index",
            limit=10,
            min_score=0.0,
            language_filter=None,
            use_hybrid=None,
            symbol_type=None,
            symbol_name=None,
        )
        assert cached is None


class TestGlobalCache:
    """Tests for global cache singleton."""

    def test_get_query_cache_returns_singleton(self):
        """get_query_cache returns same instance."""
        cache1 = get_query_cache()
        cache2 = get_query_cache()
        assert cache1 is cache2
```
  </action>
  <verify>
Run: `pytest tests/test_cache.py -v`
All tests should pass.
  </verify>
  <done>
test_cache.py exists with comprehensive tests.
Tests cover exact match, semantic match, cache miss, and invalidation.
All tests pass.
  </done>
</task>

</tasks>

<verification>
1. Run cache tests: `pytest tests/test_cache.py -v`
2. Manual cache verification:
   ```bash
   # First search (should be cache miss)
   time cocosearch search "process data" -i test-index

   # Second search (should be cache hit, much faster)
   time cocosearch search "process data" -i test-index

   # With --no-cache (should bypass)
   time cocosearch search --no-cache "process data" -i test-index
   ```
3. Cache invalidation verification:
   ```bash
   # Search to populate cache
   cocosearch search "process data" -i test-index

   # Reindex (should invalidate)
   cocosearch index ./src test-index

   # Search again (should be cache miss after reindex)
   cocosearch search "process data" -i test-index
   ```
</verification>

<success_criteria>
1. cache.py module exists with QueryCache class
2. Exact match caching works (sub-10ms for repeated identical queries)
3. Semantic caching works (cosine >0.95 threshold)
4. Cache invalidates automatically on reindex
5. --no-cache CLI flag bypasses cache
6. All unit tests pass
</success_criteria>

<output>
After completion, create `.planning/phases/33-deferred-v17-foundation/33-03-SUMMARY.md`
</output>
